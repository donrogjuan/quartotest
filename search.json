[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/hw2-post/index.html",
    "href": "posts/hw2-post/index.html",
    "title": "Hw2",
    "section": "",
    "text": "Introduction\nIn this post we are going to scrape themoviedb and try to construct a recommender system for new movies we might want to see. To do this, we will use scrapy, pandas, and matplotlib.\n\n\nScraping\nFirst, lets break down our spider class and its associated methods.\nHere we initialize our spider and give it a name: “tmdb_spider”. We initialize the link to the movie’s homepage with the f-string f\"https://www.themoviedb.org/movie/{subdir}/\".\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.subdir = subdir\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n#To be implemented below\n    \n    def parse(self, response):\n        pass\n\n    def parse_full_credits(self, response):\n        pass\n\n     def parse_actor_page(self, response):\n         pass\n\nNow we need to extract the link to the credits page where the cast can be found. Then we can srape the other films they have been in.\n\ndef parse(self, response):\n        full_url = response.urljoin(f\"/movie/{self.subdir}/cast\")\n        #self.logger.info(f\"Following cast page: {full_url}\")\n        yield scrapy.Request(url=full_url, callback=self.parse_full_credits)\n\nThe method below will use the xpath method to retrieve the list of actors in the cast, and then retreive the link to that actors page from their associated a-tag.\nThe xpath \"//*[@id='media_v4']/div/div/section[1]/ol/li\" directs to the list of cast members.\n\ndef parse_full_credits(self, response):\n        actors = response.xpath(\"//*[@id='media_v4']/div/div/section[1]/ol/li\")\n        for actor in actors:#loop through actors\n            link = actor.xpath(\".//a/@href\").get()\n            full_link = response.urljoin(link) #fixes link\n            yield scrapy.Request(url = full_link, callback = self.parse_actor_page)\n\nThis final method below will scrape the actors page for the movies hehas acted in and these these with the actors name individually in the following format:\n{\"actor\": actor_name,\"movie\": movie_name.strip()}\nWe have to grad the actors name again, so we do so from this page in the first line. We have to pull the movies the actor has been from a bdi tag, there aren’t many bdi tags on these actor pages and it is harder to grab specific ones, so we can just filter out the bdi tags we dont without to much hassle. We filter out bdi tags if their text is one of the elements in ['Personal Info', 'Known For', 'Known Credits', 'Gender', 'Birthday', 'Place of Birth', 'Also Known As']. This may not be a clean way of doing things, but such round-about methods are often necessary in the messy job of web scraping.\nTo run this, use this command in the directory of your project: scrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis will store the results of our scraping in “results.csv”.\n\ndef parse_actor_page(self, response):\n        actor_name = response.xpath(\"//*[@id='media_v4']/div/div/div[2]/div/section[1]/div/h2/a/text()\").get()\n        movies = response.xpath(\"//bdi/text()\").getall()\n        for movie_name in movies:\n            if movie_name not in ['Personal Info', 'Known For', 'Known Credits', 'Gender', 'Birthday', 'Place of Birth', 'Also Known As']:\n                yield {\n                    \"actor\": actor_name,\n                    \"movie\": movie_name.strip()\n                }\n\n\n\nData analysis\nNow we can import pandas and start looking at how to construct a recommender system from “results.csv”. We will use Harry Potter and the Philosoper’s Stone because I have watched this movie maybe three times and think its pretty good. It is also a big production with a large cast so this should give us lots of data.\n\nimport pandas as pd\ndata = pd.read_csv(\"results.csv\")\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie\n\n\n\n\n0\nDaniel Radcliffe\nHarry Potter and the Philosopher's Stone\n\n\n1\nDaniel Radcliffe\nHarry Potter and the Chamber of Secrets\n\n\n2\nDaniel Radcliffe\nHarry Potter and the Prisoner of Azkaban\n\n\n3\nDaniel Radcliffe\nHarry Potter and the Goblet of Fire\n\n\n4\nDaniel Radcliffe\nHarry Potter and the Deathly Hallows: Part 2\n\n\n...\n...\n...\n\n\n1084\nGeraldine Somerville\nThe Black Velvet Gown\n\n\n1085\nGeraldine Somerville\nPerformance\n\n\n1086\nGeraldine Somerville\nAgatha Christie's Poirot\n\n\n1087\nGeraldine Somerville\nCasualty\n\n\n1088\nGeraldine Somerville\nThe Bill\n\n\n\n\n1089 rows × 2 columns\n\n\n\nWe count the number of times the a movie is listed in our data set and sort by frequency. This should provide a good metric for movies we might like as the root of our scraping(philosopher’s stone) appears first along with other movies in the series. This makes sense because every actor in the data set was in the movie in the root, and many were rehired for the other films in the series.\n\nactor_counts = df.groupby(\"movie\")[\"actor\"].nunique()\nactor_counts = actor_counts.sort_values(ascending=False).head(20)\nactor_counts\n\nmovie\nHarry Potter and the Philosopher's Stone        32\nHarry Potter and the Chamber of Secrets         12\nDoctor Who                                       7\nHarry Potter and the Prisoner of Azkaban         5\nDay of Death                                     5\nHarry Potter and the Deathly Hallows: Part 2     4\nThe Ellen DeGeneres Show                         4\nLIVE with Kelly and Mark                         3\nMidsomer Murders                                 3\nWatch What Happens Live with Andy Cohen          3\nDancing with the Stars                           3\nPerformance                                      3\nHarry Potter and the Order of the Phoenix        3\nHarry Potter and the Half-Blood Prince           3\nHarry Potter and the Goblet of Fire              3\nSuper Gran                                       3\nHarry Potter and the Deathly Hallows: Part 1     3\nThe Kelly Clarkson Show                          3\nSchoolhouse Rock! 50th Anniversary Singalong     2\nThe View                                         2\nName: actor, dtype: int64\n\n\nTo conclude, we will plot a nice histogram of our sorted data using pyplot.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nactor_counts.head(10).plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n\nplt.xlabel(\"Movie\")\nplt.ylabel(\"Number of Actors\")\nplt.title(\"Number of Actors in Each Movie\")\nplt.xticks(rotation=45, ha=\"right\") #rotate labels for readability\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "Data Visualization\nHello! We our going to visualize the “palmer penguins” data set. As is common practice, we will use Pandas for storing and manipulating our data in a dataframe and MatPlotLib and Numpy for viualization.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy\n\nNow, we can fetch the data and store it in a Pandas dataframe.\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\ndf = pd.DataFrame(penguins) #create dataframe\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nCleaning our Data\nWe want to see if the size of the penguins differs by species in any way, and visualize this difference. First, we should clean our data.\nWe only need the Penguin’s species, sex, culmen length, and body mass for our purposes, so we will create a new dataframe with just these columns. Also, we should get rid of NaN values. Also, lets shorten the species names.\n\ncolumns = [\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"]\ndf = df[columns] #create new df\ndf = df.dropna() #drop NANs\ndf[\"Species\"] = df[\"Species\"].str[:5] #shortens to first five letters of the Species name\ndf.head()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nBody Mass (g)\nSex\n\n\n\n\n0\nAdeli\n181.0\n3750.0\nMALE\n\n\n1\nAdeli\n186.0\n3800.0\nFEMALE\n\n\n2\nAdeli\n195.0\n3250.0\nFEMALE\n\n\n4\nAdeli\n193.0\n3450.0\nFEMALE\n\n\n5\nAdeli\n190.0\n3650.0\nMALE\n\n\n\n\n\n\n\n\n\nPlotting\nFinally, we can begin plotting our data. We want to seperate our data into teo plots by sex. Also, we want to color the different species. To do this we, define the function plot_species that takes the axis of our subplot and the sex of the penguins we want to plot.\n\nfig, axes = plt.subplots(1,2)\n\ndef plot_species(axis,sex):\n    df.groupby(\"Species\").apply(\n        lambda group: axes[axis].scatter(\n            group[group[\"Sex\"] == sex][\"Flipper Length (mm)\"],\n            group[group[\"Sex\"] == sex][\"Body Mass (g)\"],\n            label=group[\"Species\"].iloc[0]\n            )\n    )\n    axes[axis].legend()\n    axes[axis].set_xlabel(\"Flipper Length (mm)\")\n    axes[axis].set_ylabel(\"Body Mass (g)\")\n    axes[axis].set_title(f\"Species: {sex}\")\n    \nplot_species(0,\"MALE\")\nplot_species(1,\"FEMALE\")\n\n/var/folders/bk/2x4cd_5s4sbfc4zg448d5cbh0000gn/T/ipykernel_64925/351063246.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df.groupby(\"Species\").apply(\n/var/folders/bk/2x4cd_5s4sbfc4zg448d5cbh0000gn/T/ipykernel_64925/351063246.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df.groupby(\"Species\").apply(\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nNow that we have visualized are data, we can interpret its structure. It is clear that the Gento penguins are almost always bigger than the other penguins both in weight and flipper length."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Hw2\n\n\n\n\n\n\nweek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\nRoger\n\n\n\n\n\n\n\n\n\n\n\n\nCreating posts\n\n\n\n\n\n\nweek 3\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nRoger Kozlyuk\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 23, 2025\n\n\nRoger Kozlyuk\n\n\n\n\n\n\nNo matching items"
  }
]