[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to my Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/hw4-post/hw4_16b 2.html",
    "href": "posts/hw4-post/hw4_16b 2.html",
    "title": "Modeling the Heat Dispersion",
    "section": "",
    "text": "In this notebook we will be looking modeling how heat travels on the plane.REALLY we will be looking at how we can construct the same model in different ways to acheive FASTER results.\n#Set the size of our plane and the error.\n\nN = 101\nepsilon = 0.2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n# construct initial condition: 1 unit of heat at midpoint. \nu0 = np.zeros((N, N))\nu0[int(N/2), int(N/2)] = 1.0\nplt.imshow(u0)"
  },
  {
    "objectID": "posts/hw4-post/hw4_16b 2.html#initial-simulations",
    "href": "posts/hw4-post/hw4_16b 2.html#initial-simulations",
    "title": "Modeling the Heat Dispersion",
    "section": "Initial Simulations",
    "text": "Initial Simulations\nIn these first simulations, matrix multiplication will be used to change the state of the simulation to reflect how the heat travels through the plane. u0 will represent the plane in it’s initial state and A will be the update matrix.\n\ndef advance_time_matvecmul(A, u, epsilon):\n    \"\"\"Advances the simulation by one timestep, via matrix-vector multiplication\n    Args:\n        A: The 2d finite difference matrix, N^2 x N^2. \n        u: N x N grid state at timestep k.\n        epsilon: stability constant.\n\n    Returns:\n        N x N Grid state at timestep k+1.\n    \"\"\"\n    N = u.shape[0]\n    u = u + epsilon * (A @ u.flatten()).reshape((N, N))\n    return u\n\n\ndef get_A(N):\n    \"\"\"Generates the update matrix\"\"\"\"\n    n = N * N\n    diagonals = [-4 * np.ones(n), np.ones(n-1), np.ones(n-1), np.ones(n-N), np.ones(n-N)]\n    diagonals[1][(N-1)::N] = 0\n    diagonals[2][(N-1)::N] = 0\n    A = np.diag(diagonals[0]) + np.diag(diagonals[1], 1) + np.diag(diagonals[2], -1) + np.diag(diagonals[3], N) + np.diag(diagonals[4], -N)\n    return A\n\n\ndef simulation1(num_iterations, u, A, epsilson):\n    \"\"\"Simulates heat flow\"\"\"\n    for i in range(num_iterations):\n        u = advance_time_matvecmul(A, u, epsilon)\n        if i%300 == 0:\n            plt.imshow(u)\n\n\nA = get_A(N)\n\n\nsimulation1(2700, u0, A, epsilon)\n\n\n\n\n\n\n\n\n\nVery Slow…\nThis took about 7 minutes to run. We can speed it up using a sparse encoding of our update matrix from jax.\n\nfrom jax.experimental import sparse\nimport jax.numpy as jnp\nfrom jax import jit\n\n\ndef get_sparse_A(N):\n    \"\"\"Genereates sparse matrix\"\"\"\n    n = N * N\n    diagonals = [-4 * jnp.ones(n), jnp.ones(n-1), jnp.ones(n-1), jnp.ones(n-N), jnp.ones(n-N)]\n    diagonals[1] = diagonals[1].at[(N-1)::N].set(0)\n    diagonals[2] = diagonals[2].at[(N-1)::N].set(0)\n    A = jnp.diag(diagonals[0]) + jnp.diag(diagonals[1], 1) + jnp.diag(diagonals[2], -1) + jnp.diag(diagonals[3], N) + jnp.diag(diagonals[4], -N)\n    A = sparse.BCOO.fromdense(A)\n    return A\n\n\nA_sparse = get_sparse_A(N)\n\n\nu0 = jnp.zeros((N, N))\nu0 = u0.at[int(N/2), int(N/2)].set(1.0)\n\n\ndef simulation2(num_iterations, u, A, epsilson):\n    for i in range(num_iterations):\n        u = advance_time_matvecmul(A, u, epsilon)\n        #if i%300 == 0:\n         #   plt.imshow(u)\n\n\nsimulation2(2700, u0, A, epsilon)\n\n\n\n\n\n\n\n\n\n\nCompare Times\ncomparing the times, we see the sparse encoding helps a lot.\n\nimport timeit\n\ntime_taken1 = timeit.timeit(lambda: simulation1(2700, u0, A, epsilon), number=1)\nprint(f\"Elapsed time: {time_taken1:.6f} seconds\")\n\nElapsed time: 361.131413 seconds\n\n\n\n\n\n\n\n\n\n\ntime_taken2 = timeit.timeit(lambda: simulation2(2700, u0, A_sparse, epsilon), number=1)\nprint(f\"Elapsed time: {time_taken2:.6f} seconds\")\n\nElapsed time: 7.645487 seconds"
  },
  {
    "objectID": "posts/hw4-post/hw4_16b 2.html#numpy-implemenationno-matrix-multiply",
    "href": "posts/hw4-post/hw4_16b 2.html#numpy-implemenationno-matrix-multiply",
    "title": "Modeling the Heat Dispersion",
    "section": "Numpy Implemenation(No Matrix Multiply)",
    "text": "Numpy Implemenation(No Matrix Multiply)\nNow we try a numpy implementation of our simulation using np.roll to remove the need for expensive matrix multiplication when updating.\n\ndef advance_time_numpy(u, epsilon):\n    return u + epsilon *  (np.roll(u, 1, axis=0) + np.roll(u, -1, axis=0) + np.roll(u, 1, axis=1) + np.roll(u, -1, axis=1) - 4 * u)\n\n\ndef simulation3(num_iterations, u, epsilson):\n    for i in range(num_iterations):\n        u = advance_time_numpy(u, epsilon)\n        if i%300 == 0:\n            plt.imshow(u)\n\n\nsimulation3(2700,u0,epsilon)\n\n\n\n\n\n\n\n\n\ntime_taken3 = timeit.timeit(lambda: simulation3(2700, u0, epsilon), number=1)\nprint(f\"Elapsed time: {time_taken3:.6f} seconds\")\n\nElapsed time: 0.307338 seconds\n\n\n\n\n\n\n\n\n\n\nFinally we implement the previous simulation using JAX.\n\n\nfrom jax import jit\n@jit\ndef advance_time_jax(u, epsilon):\n    return u + epsilon *  (jnp.roll(u, 1, axis=0) + jnp.roll(u, -1, axis=0) + jnp.roll(u, 1, axis=1) + jnp.roll(u, -1, axis=1) - 4 * u)\n\n\ndef simulation4(num_iterations, u, epsilson):\n    for i in range(num_iterations):\n        \n        u = advance_time_jax(u, epsilon)\n        if i%300 == 0:\n            plt.imshow(u)\n\n\nsimulation4(2700,u0,epsilon)\n\n\n\n\n\n\n\n\n\ntime_taken4 = timeit.timeit(lambda: simulation4(2700, u0, epsilon), number=1)\nprint(f\"Elapsed time: {time_taken4:.6f} seconds\")\n\nElapsed time: 0.065000 seconds"
  },
  {
    "objectID": "posts/hw4-post/hw4_16b 2.html#analysis-of-time-data",
    "href": "posts/hw4-post/hw4_16b 2.html#analysis-of-time-data",
    "title": "Modeling the Heat Dispersion",
    "section": "Analysis of time data",
    "text": "Analysis of time data\n\ntime_data = {\"advance_time_matvecmul\":time_taken1,\"advance_time_matvecmul(sparse)\":time_taken2,\"advance_time_numpy\":time_taken3,\"advance_time_jnp\":time_taken4}\n\n\ntime_data\n\n{'advance_time_matvecmul': 361.13141283299774,\n 'sparse': 7.645487499423325,\n 'advance_time_numpy': 0.28845562506467104,\n 'advance_time_jnp': 0.06499958410859108}\n\n\n\nimport pandas as pd\ntimes = pd.DataFrame(time_data, index=[\"Times\"])\n\n\ntimes\n\n\n\n\n\n\n\n\nadvance_time_matvecmul\nadvance_time_matvecmul(sparse)\nadvance_time_numpy\nadvance_time_jnp\n\n\n\n\nTimes\n361.131413\n7.645487\n0.307338\n0.065\n\n\n\n\n\n\n\n\ntimes.T.plot()\nplt.xticks(rotation = 45)\nplt.ylabel(\"Time in seconds\")\nplt.title(\"Computation Times\")\n\nText(0.5, 1.0, 'Computation Times')\n\n\n\n\n\n\n\n\n\n\nnp.log(times.T).plot()\nplt.xticks(rotation = 45)\nplt.ylabel(\"Log(Time in seconds)\")\nplt.title(\"Log of Computation Times\")\n\nText(0.5, 1.0, 'Log of Computation Times')"
  },
  {
    "objectID": "posts/hw4-post/hw4_16b 2.html#conclusion",
    "href": "posts/hw4-post/hw4_16b 2.html#conclusion",
    "title": "Modeling the Heat Dispersion",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the “Log of Computation Times” graph, it is clear that time taken to execute the simulation is reduced by a factor of ten for each optimization. At the end, the improvements start to taper off, but this was expected. Overall, heat dispersion simulations can be done very quickly in this setting, but if you make the wrong design choices, you could be waiting around for a while."
  },
  {
    "objectID": "posts/hw2-post/index.html",
    "href": "posts/hw2-post/index.html",
    "title": "Hw2",
    "section": "",
    "text": "Introduction\nIn this post we are going to scrape themoviedb and try to construct a recommender system for new movies we might want to see. To do this, we will use scrapy, pandas, and matplotlib.\n\n\nScraping\nFirst, lets break down our spider class and its associated methods.\nHere we initialize our spider and give it a name: “tmdb_spider”. We initialize the link to the movie’s homepage with the f-string f\"https://www.themoviedb.org/movie/{subdir}/\".\n\nimport scrapy\n\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n\n    def __init__(self, subdir=\"\", *args, **kwargs):\n        self.subdir = subdir\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\n\n#To be implemented below\n    \n    def parse(self, response):\n        pass\n\n    def parse_full_credits(self, response):\n        pass\n\n     def parse_actor_page(self, response):\n         pass\n\nNow we need to extract the link to the credits page where the cast can be found. Then we can srape the other films they have been in.\n\ndef parse(self, response):\n        full_url = response.urljoin(f\"/movie/{self.subdir}/cast\")\n        #self.logger.info(f\"Following cast page: {full_url}\")\n        yield scrapy.Request(url=full_url, callback=self.parse_full_credits)\n\nThe method below will use the xpath method to retrieve the list of actors in the cast, and then retreive the link to that actors page from their associated a-tag.\nThe xpath \"//*[@id='media_v4']/div/div/section[1]/ol/li\" directs to the list of cast members.\n\ndef parse_full_credits(self, response):\n        actors = response.xpath(\"//*[@id='media_v4']/div/div/section[1]/ol/li\")\n        for actor in actors:#loop through actors\n            link = actor.xpath(\".//a/@href\").get()\n            full_link = response.urljoin(link) #fixes link\n            yield scrapy.Request(url = full_link, callback = self.parse_actor_page)\n\nThis final method below will scrape the actors page for the movies hehas acted in and these these with the actors name individually in the following format:\n{\"actor\": actor_name,\"movie\": movie_name.strip()}\nWe have to grad the actors name again, so we do so from this page in the first line. We have to pull the movies the actor has been from a bdi tag, there aren’t many bdi tags on these actor pages and it is harder to grab specific ones, so we can just filter out the bdi tags we dont without to much hassle. We filter out bdi tags if their text is one of the elements in ['Personal Info', 'Known For', 'Known Credits', 'Gender', 'Birthday', 'Place of Birth', 'Also Known As']. This may not be a clean way of doing things, but such round-about methods are often necessary in the messy job of web scraping.\nTo run this, use this command in the directory of your project: scrapy crawl tmdb_spider -o results.csv -a subdir=671-harry-potter-and-the-philosopher-s-stone\nThis will store the results of our scraping in “results.csv”.\n\ndef parse_actor_page(self, response):\n        actor_name = response.xpath(\"//*[@id='media_v4']/div/div/div[2]/div/section[1]/div/h2/a/text()\").get()\n        movies = response.xpath(\"//bdi/text()\").getall()\n        for movie_name in movies:\n            if movie_name not in ['Personal Info', 'Known For', 'Known Credits', 'Gender', 'Birthday', 'Place of Birth', 'Also Known As']:\n                yield {\n                    \"actor\": actor_name,\n                    \"movie\": movie_name.strip()\n                }\n\n\n\nData analysis\nNow we can import pandas and start looking at how to construct a recommender system from “results.csv”. We will use Harry Potter and the Philosoper’s Stone because I have watched this movie maybe three times and think its pretty good. It is also a big production with a large cast so this should give us lots of data.\n\nimport pandas as pd\ndata = pd.read_csv(\"results.csv\")\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nactor\nmovie\n\n\n\n\n0\nDaniel Radcliffe\nHarry Potter and the Philosopher's Stone\n\n\n1\nDaniel Radcliffe\nHarry Potter and the Chamber of Secrets\n\n\n2\nDaniel Radcliffe\nHarry Potter and the Prisoner of Azkaban\n\n\n3\nDaniel Radcliffe\nHarry Potter and the Goblet of Fire\n\n\n4\nDaniel Radcliffe\nHarry Potter and the Deathly Hallows: Part 2\n\n\n...\n...\n...\n\n\n1084\nGeraldine Somerville\nThe Black Velvet Gown\n\n\n1085\nGeraldine Somerville\nPerformance\n\n\n1086\nGeraldine Somerville\nAgatha Christie's Poirot\n\n\n1087\nGeraldine Somerville\nCasualty\n\n\n1088\nGeraldine Somerville\nThe Bill\n\n\n\n\n1089 rows × 2 columns\n\n\n\nWe count the number of times the a movie is listed in our data set and sort by frequency. This should provide a good metric for movies we might like as the root of our scraping(philosopher’s stone) appears first along with other movies in the series. This makes sense because every actor in the data set was in the movie in the root, and many were rehired for the other films in the series.\n\nactor_counts = df.groupby(\"movie\")[\"actor\"].nunique()\nactor_counts = actor_counts.sort_values(ascending=False).head(20)\nactor_counts\n\nmovie\nHarry Potter and the Philosopher's Stone        32\nHarry Potter and the Chamber of Secrets         12\nDoctor Who                                       7\nHarry Potter and the Prisoner of Azkaban         5\nDay of Death                                     5\nHarry Potter and the Deathly Hallows: Part 2     4\nThe Ellen DeGeneres Show                         4\nLIVE with Kelly and Mark                         3\nMidsomer Murders                                 3\nWatch What Happens Live with Andy Cohen          3\nDancing with the Stars                           3\nPerformance                                      3\nHarry Potter and the Order of the Phoenix        3\nHarry Potter and the Half-Blood Prince           3\nHarry Potter and the Goblet of Fire              3\nSuper Gran                                       3\nHarry Potter and the Deathly Hallows: Part 1     3\nThe Kelly Clarkson Show                          3\nSchoolhouse Rock! 50th Anniversary Singalong     2\nThe View                                         2\nName: actor, dtype: int64\n\n\nTo conclude, we will plot a nice histogram of our sorted data using pyplot.\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 5))\nactor_counts.head(10).plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"black\")\n\nplt.xlabel(\"Movie\")\nplt.ylabel(\"Number of Actors\")\nplt.title(\"Number of Actors in Each Movie\")\nplt.xticks(rotation=45, ha=\"right\") #rotate labels for readability\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Creating posts",
    "section": "",
    "text": "Data Visualization\nHello! We our going to visualize the “palmer penguins” data set. As is common practice, we will use Pandas for storing and manipulating our data in a dataframe and MatPlotLib and Numpy for viualization.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy\n\nNow, we can fetch the data and store it in a Pandas dataframe.\n\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)\ndf = pd.DataFrame(penguins) #create dataframe\ndf.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\n\nCleaning our Data\nWe want to see if the size of the penguins differs by species in any way, and visualize this difference. First, we should clean our data.\nWe only need the Penguin’s species, sex, culmen length, and body mass for our purposes, so we will create a new dataframe with just these columns. Also, we should get rid of NaN values. Also, lets shorten the species names.\n\ncolumns = [\"Species\", \"Flipper Length (mm)\", \"Body Mass (g)\", \"Sex\"]\ndf = df[columns] #create new df\ndf = df.dropna() #drop NANs\ndf[\"Species\"] = df[\"Species\"].str[:5] #shortens to first five letters of the Species name\ndf.head()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nBody Mass (g)\nSex\n\n\n\n\n0\nAdeli\n181.0\n3750.0\nMALE\n\n\n1\nAdeli\n186.0\n3800.0\nFEMALE\n\n\n2\nAdeli\n195.0\n3250.0\nFEMALE\n\n\n4\nAdeli\n193.0\n3450.0\nFEMALE\n\n\n5\nAdeli\n190.0\n3650.0\nMALE\n\n\n\n\n\n\n\n\n\nPlotting\nFinally, we can begin plotting our data. We want to seperate our data into teo plots by sex. Also, we want to color the different species. To do this we, define the function plot_species that takes the axis of our subplot and the sex of the penguins we want to plot.\n\nfig, axes = plt.subplots(1,2)\n\ndef plot_species(axis,sex):\n    df.groupby(\"Species\").apply(\n        lambda group: axes[axis].scatter(\n            group[group[\"Sex\"] == sex][\"Flipper Length (mm)\"],\n            group[group[\"Sex\"] == sex][\"Body Mass (g)\"],\n            label=group[\"Species\"].iloc[0]\n            )\n    )\n    axes[axis].legend()\n    axes[axis].set_xlabel(\"Flipper Length (mm)\")\n    axes[axis].set_ylabel(\"Body Mass (g)\")\n    axes[axis].set_title(f\"Species: {sex}\")\n    \nplot_species(0,\"MALE\")\nplot_species(1,\"FEMALE\")\n\n/var/folders/bk/2x4cd_5s4sbfc4zg448d5cbh0000gn/T/ipykernel_64925/351063246.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df.groupby(\"Species\").apply(\n/var/folders/bk/2x4cd_5s4sbfc4zg448d5cbh0000gn/T/ipykernel_64925/351063246.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  df.groupby(\"Species\").apply(\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\nNow that we have visualized are data, we can interpret its structure. It is clear that the Gento penguins are almost always bigger than the other penguins both in weight and flipper length."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/hw4-post/index.html",
    "href": "posts/hw4-post/index.html",
    "title": "Modeling Heat Dispersion",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "myblog",
    "section": "",
    "text": "Modeling the Heat Dispersion\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Heat Dispersion\n\n\n\n\n\n\nhw5\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\nRoger Kozlyuk\n\n\n\n\n\n\n\n\n\n\n\n\nHw2\n\n\n\n\n\n\nweek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 10, 2025\n\n\nRoger Kozlyuk\n\n\n\n\n\n\n\n\n\n\n\n\nCreating posts\n\n\n\n\n\n\nweek 3\n\n\nexample\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nRoger Kozlyuk\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 26, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to my Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 23, 2025\n\n\nRoger Kozlyuk\n\n\n\n\n\n\nNo matching items"
  }
]